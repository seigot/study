readme

---

```
But I, I'm not. I'm not honestly privy to some of the mechanics on the back end. Yeah, no, no worries. I was asking, because I thought I saw a person in here. The name of the account with no camera, says UCSD underscore RSM micro MBA. So I assumed it, perhaps?
00:22
Eric, um, there's a couple links in the chat. I want to say the the one I put in is the one that I use to get directly to my dashboard. I think the first one Muhammad that one might be right, too.
00:43
Uh, you had a question regarding canvas. I'm sorry, Professor. I'll just see what the questions and I'll let you get better. No worries, a confusion with the canvas. Could you please repeat the question? Yeah, no worries. I got the link u. Muhammad, I shared a lake. It goes to inside Janice.
01:02
I was just looking for clarity on which things we need to fill out and by when. Today had a link in it for a canvas tutorial, but actually the link goes to ucsd.zoom.us, which I assume is. Yeah, that that might have been a mistake. Uh, so the link is, I think, shared on the, uh, like the chat, uh?
01:23
But inside, that there's like a discussion or something that should have come up, uh, which has like a 24-hour deadline. So forth? Uh, from the point when the class is over, so it was, like, next day four o'clock would be. The deadline is what I believe. I think Marva, you could also like something.
01:39
There's any country you know and that, but yeah, it's just. There's, I think, a 24-hour deadline for that. Oh, I will just send a follow-up email to like clarify any of that so that we can just get started with the session. Or you could continue the conversation oversight so that the professor can become processed.
01:59
We're fine. I think it's important that everybody's clear about what so. Is this a discussion like on the topic that I'm talking about, or this is from the prior session? Uh, the reason this is the logistics for the, uh, like, yeah, it's just Logistics, and I'm looking at now.
02:13
And even though I did the lecture? Uh. The pre-assignment. It still has zero points, so maybe that's just getting caught up. I just don't want to miss the credit as. Okay, so it sounds like there might be a couple things to follow up on after the session. Um, with that, how about, uh?
02:31
We jump into the topic before this afternoon, and I guess, just barely into the evening. I hope everybody can see my scream, and he not my screen my screen and hear me okay. I really appreciate a bunch of you being on Zoom and on on camera. Still remember the very first time I taught on Zoom, which was right at the start of recording stop, and nobody had their cameras on recordings, talking to a wall, and some people are really good at that.
02:59
And one of my colleagues said, I just talked to the green light on my MacBook. It works really well, and I can't. I can't do that. I don't know how to do that. So when I'm when I'm, you know, talking to students, even on Zoom, let's say that my camera's here, and that person's video is over there, I'm talking that way.
03:17
I find it very difficult to do anything else, so thank you so much for being here. I love talking about this topic, so if. If I get carried away, feel free to stop me anytime I have more slides than I can possibly get through. And that's a, that's one of my main flaws.
03:32
As an instructor, I would say, I have, I have more material than we ever end up being able to cover. But again, I'm happy to stop anytime and dive into any topic that you are interested in or want to want to follow up on and I'll hope we can make it somewhat interactive even though it's on Zoom again.
03:48
Please stop me anytime. Put something in the chat. I've got the chat open here, so I should be able to see and what you're putting in there, but my preference would be that you just unreach yourself and just share your opinions, your insights, uh, experiences. All right. So this is the the general topic area business analytics, and you can see the title.
04:05
I'll just hide myself for a second. Aka data science for business. I'll talk a little bit more about that connection in a second. This is an area that I've been teaching in and doing researching for a long time. And so again. That's one of the reasons why there's a there's a lot of material.
04:22
So, just a quick. View on who I am. I have a master of science from the University of Corona in the Netherlands. I got my PhD in living in Belgium. My research area is in marketing Effectiveness and more and more these days in generator VI and how it applies in education and data analysis generated for quite some time now.
04:41
Before that, I was at the public school of management at Northwestern co-founder, faculty director of the Natural Science and business analytics program. That's one reason why I'm why I'm doing this this talk today, and I just recently converted from a research track to a teaching track. I've been so involved with the master of science and business analytics program for over a decade now that it just seems where I can create the most value for for the school.
05:03
Um, and honestly, with everything that's going on in January VI. I honestly found some of my old research a little boring these days. There's just so much exciting stuff and new stuff happening in this field of generator VI that I really just want to focus on that and bringing that to our students because they very much need to leverage that in their, um, in their learning as well as in their in their employment.
05:25
Now, I think that's also always challenging for people is when they see my name is like, how do we pronounce that? Is there anybody who spent any time with the Netherlands or Belgium? And I give that a shot. Oh, go ahead, Melanie. Hi, I just I went to Bruges for a few days, and it was amazing.
05:43
Okay, okay. Did you meet any you might not have met anybody with that particular letter combination. I think it's pretty much only happens in the Netherlands or Belgium. Um, and the the closest word in the English language in terms of pronunciation is n-i-c-e. So, the name nice is how you pronounce it, exactly.
06:02
So, I'm Professor nice, although my students don't always agree with that assessment. All right, so I teach you a bunch of different things at the at Rady. So, what I'm about to start teaching again is the AI assistant math and programming class. I teach customer analytics and generator VI.
06:20
I am often involved in our Capstone project, and I just finished teaching this spring. A first version of a course on deep learning and generative AI for business, which was which was a lot of fun. And then there's a bunch of boot camps and sort of computing technology. Related workshops that I do?
06:35
Um, I don't do a ton of executive Education just because I, I really enjoy directly interacting with the students in our school in our program, uh, but on occasion I'll visit a company here or there and do some Executive Education on the side as well. Now! I'd like to think I'm a decent educator by this point.
06:52
In my career, I started in 2001, right after my, my PhD at Kellogg, Kellogg is a as a as a big, a big MBA program, and I figured, hey, I could present for an hour and a half pretty well on my research. So, how hard can it be right to present for 60 or 70 mbas for for a couple of hours?
07:12
Turns out it was pretty darn hard. Uh, thanks for me. So, my first couple of years were a little rough, and when I, when I was talking about business analytics at that time. To the students, this was sort of the facial expression that a lot of them would have in the classroom.
07:23
They really didn't like me. They didn't like the topic that I was talking about, and at that point it was more like. Why are you teaching me all of this quantitative stuff? My analyst will take care of that for me. And those days are long, long gone. Uh, now, over time, I think students start to appreciate some of the more quantitative components of the MBA program, and I probably got a little bit better at as a as an instructor as well, so their facial expressions became a bit more neutral.
07:48
And these days, I I'd like to think I see more sort of happy faces in the class they're interested in the topic, and they're interested in working with me and learning as much as they can about a bunch of booming areas in the areas of AI and generative AI.
08:01
Um, and I think one of the key key reasons why students have become more focused and more interested in analytics over time is the massive amount of data that gets generated by by us by systems, and uh, just as a as a nice overview of that, because a graph that you can find online, there's a variety of them.
08:20
So, what happens in a minute online? Under the 67 videos watched on Tick Tock, uh, in the U.S alone. The amount of information that goes back and forth that we track that we store is is enormous and. It's, it's true that there's more data that's been generated in the last few years than in the entire history of the human race combines.
08:44
It just keeps going up and up and up so incredibly rapidly. Uh, so that now over a 1.7 megabytes of new information per second per person again, enormous amount. And so, when I saw that number. For the first time, the 1.7 megabytes I had to think of this. Can you even tell me what that is, you can put it in a chat?
09:05
Puppy disk. So, I'm waiting for the day when somebody says that's the save icon. Right, cuz, who knows what I, I mean, who? Who? Here, amongst us has seen a floppy disk in the last decade. Oh, really. Wow, you got your garage, or you're adding, I thought you meant, like, that's right.
09:25
Yeah, like, from 20 years ago, yes, yes. Um, and so again, it is still around. It's by the way, of course not. An actual floppy disk, even though that was the name that was used because before, right before this, we had the five and a quarter inch floppy disk that actually, you could bend a little bit.
09:41
You were, if you were careful, right, but do you remember what the, what the capacity was for those? 1.44 MB. So more than that per second per person. It's enormous, right? And so that's one of the reasons why I think it's become a huge amount of interest in just data.
10:00
In general, there's so much happening so much data being generated, and there's still so many people thinking, well, what do we do with all this, I know we have a bunch of stuff, but can we actually use it for something? I said, just to give you a sense of again all the data that we generate ourselves as a group all the time, anytime.
10:16
There's a credit card transaction. We do an online purchase. We're surfing online audio, video streaming, reactions to mail and email offers customer support calls that are probably automatically transcribed on the back end and processed. The self-provided information. Like, if we still get surveys and forms and things like that, that we have to fill out all the time, location information.
10:34
Obviously, iot social media posts videos likes you name. I mean, they're so incredibly much data that we that we generate on a day-to-day basis, and that is, in many ways, very valuable to organizations. Now, some organizations don't use that data particularly well, and some of them use it for just plain spamming.
10:51
But there's a lot of opportunity with all of this data for companies to build products and services. And share those with company. I'm sorry with could be B2B. Great could be other companies, but also with their consumers in a way that generates Mutual benefit, right? So, that's so, what I would say is that it's almost the definition of marketing is finding it at transaction that you can repeat over time where you're mutually beneficial, developing a mutually beneficial relationship.
11:15
Okay, and there's a huge amount of value in this data. It's just that there's so much it can be very difficult to figure out. What pages do we focus on? What do we actually use? There's just some examples, right? So you also see this in the news with some very good I already.
11:26
You've probably seen one of these. When there's a? I don't remember what these are called hurricane hurricanes. Yes. Uh, tornadoes and hurricanes are not things we generally find in the Netherlands. These are things that I just started hearing about with yes, like some areas you don't want to go okay.
11:42
Uh, so this is when a hurricane hits, let's say, somewhere in Florida. Uh, Walmart knows in advance. What do they need to do to make sure that in those areas? There's enough of everything that people want, right? So the surrounding Earth that are not likely to be impacted, what should they be pushing over into those areas?
11:58
And of course you would expect like water is going to be one of the one one or two top things, uh, but it seems in the in the top five. There are also strawberry Pop-Tarts and beer. Doesn't seem like a great combination to me, but based on data that they've been tracking over many years, and many of these instances, they're like, hey, that's where this type of stuff that people run out of in these instances.
12:17
Like to make sure that they're well stocked up with all of these vital vital products. Okay, uh, and again, it's a very, very valid use of data. This is one of my my favorites. Uh, this is from Netflix when they were going into the Netherlands and the Netherlands has some similarities to the US and other markets that also have its own sort of unique interests in in, um, in shows and in movies, one of the things that Netflix did when they rolled out into the Netherlands that they would look at pirate sites.
12:46
I kind of see what were shows that were particularly tops that are on those again. Get a sense of what's the? What's the taste of particular demographics within that country that they could then utilize those insights to figure out what are the most likely things that we want to offer on our service.
13:00
In this particular Market, it's not a market that they knew, and so they utilized the information as best they could now. This doesn't mean that because something is.
13:12
Me Market and seeing sort of. What can we understand about these customers as we're moving into this new industry New Market? And then a very common one instead of the optimization area. We learn from from experience. If you think about the traveling salesman problem and seeing some of you have heard of that before.
13:27
There's an enormous number of routes. That's let's say UPS could take to deliver all of its products, and what are the ways they can do that most efficiently? And one of the things that they came up with what's labeled here as counterintuitive routes is that they decided that they were going to try to limit left turns as much as possible.
13:46
Obviously can eliminate them completely. Otherwise, you're going to be going either just straight or making circles. But why would they try to avoid left-hand terms? It's particularly in the US. Faster to make a right turn. You don't have to wait that the lights turn left exactly right, so you can turn on right here, even in red in most instances, and so that might lead you to a faster route, even though technically you might have to, you know.
14:09
It might be a little bit of a longer round, but it could be more efficient, right? So there's less idle time, and you might be able to get to your destination more quickly if you sort of eliminate. A few of these left turns that that might leave you stuck at a traffic light for a long time.
14:22
Just interesting phenomena that pop up as you explore data and try to optimize based on it. I'm assuming you all have in your experiences either in work or in study or wherever else, this this sense that there's a massive amount of data. We just talked about this sort of Internet minute and all the data that gets generated, uh.
14:40
But one thing that is still pretty much the same is the first time I saw the statistic. Is that a really small percentage of all the data that is generated is actually actively used? And at the right of reason. For this, for one we sometimes collect data that we're just not going to use because it's either not useful or just the quantity is difficult to manage, but another reason is just that.
15:03
The systems and people aren't really in place to actively utilize that data. Right. So sometimes we collect things we don't really need, and even if we do collect things we don't need, we don't have the people, the time, the resources, the methodologies to actually Implement sort of decision making based on that data.
15:17
So that's still sort of an unsolved area, but I'd argue that generative VI can be a big help in this area as well. In addition to, of course, training people to understand and be be more sort of interested in, uh, business analytics and data analytics. So that brings me back to data science versus business analytics.
15:35
And so, when we started out the business analytics program, it was clear that this was the best name for our our product, our service, our our master's degree. But since that time, business analytics has sort of gotten more of a label of, well, that just means you do stuff with Tableau and Excel, and I'd argue that that's not just what it is.
15:54
It's a part of it, but there's much more to it. And so the the bigger discipline in some ways might be considered by people to be data science, and I just want to compare how we think about business analytics versus data science. So, this is a definition from. Um, from Wikipedia.
16:09
Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data and apply knowledge and actual insights from data across a broad range of energy. And so that's a that's a fine, perfectly fine definition. This is a definition that I would use for business models and, and just to save you the trouble of reading through the whole thing.
16:31
It's the exact same as the previous one. The only things that have changed out is the first two words and the last. Right. So, if you think about business analytics, the way, the way we sort of Envision it and the way I think it should be applied in Industry is we're using all the same tools and Technologies that you might find in a data science program.
16:48
Or if you do a search on data science, I put all the same same types of methodologies. But our focus is just much more squarely on solving a business problem. Now, that doesn't mean that we might not sort of venture into different areas that are generally more studied in data science, but our focus is very much, uh, you know, more, more specific to specific problems, specific business problems.
17:08
Okay, so very, very similar in many different ways, and that's why we're also thinking about, uh, sort of undergraduate degree that may happen at some point at UCSD, which would be business data science. Data science is very broad. They can sometimes have what I feel is a little bit like toy examples where we're just looking at a variety of different application areas, but I think in, in, if you specify a more specific business domain, you can go into more depth into how do I solve a problem with analytics rather than thinking about the methodologies?
17:37
Is the hammer that you can just smash all kinds of different things from? Okay, so I hope that that makes sense to you again. Business analytics definitely involves things like Tableau and Excel, and they're very valuable tools, but there's an enormous amount of other activities you can engage in.
17:51
That would help at support business decision making within the area of Interlands. I'll just give you an example of what I'm talking about here. This is something that you might see in a temporal online if you start clear science introduction, or if you do like an introduction, data science.
18:07
Class of some sort. You would see a bunch of pictures. Some of them are cats. Some of them are not cats. So this is often referred to as supervised learning. It just means there's a bunch of images that I have. I'm going to look at those. A human is likely to look at those, or maybe some other system, and it's going to label them as either being a cat or not account, so I have an outcome an outcome variable that gives me information about what this thing is.
18:27
I'm going to try to build a model that can take the data from the images and then connect that to the outcome, which is, is this a cat or not account? And in a data science sort of perspective. You might not necessarily care about what exactly the data is that's going into it.
18:42
I just want to know. Can I utilize that information to make a prediction, and how well can I do that? And there's a variety of different models that you could apply to that? Uh, sorry, I'm just going to look at the chat here, just like when I'm unable to.
18:56
But I'm able to prove I'm dealing with the culture sort of. I would say that that, uh? There's this joke about why in national parks? They have trouble with garbage cans. Uh, they are trying to build garbage cans. That would be sort of animal proof, but yet usable by humans.
19:15
And I think the quote is something along the lines that there's there's an overlapping area between sort of the the smartest animals, and then maybe the less smart humans in terms of their ability to utilize these. So, if you make them so that animals can't get in, a bunch of humans are not going to be able to utilize this thing either.
19:30
Thank you! So, um, I would say that. These days, I'm not really sure the cops should really have any effect anymore in any way. You could set up a system to solve those with too much with that too much difference. Um. Okay, but so again, this is this. Is this is considered supervised learning?
19:47
There's an outcome variable cat. No cat somebody's labeled those for you, and there's their sort of input data explanatory variables if you will. In a traditional like regression sense, we're trying to make a connection, and that's sort of a pretty interesting exercise to work with. But this might be a more interesting one from a business analytics perspective, right?
20:04
So we could look at variety of different areas for marketing to supply to management. This would be one in the area of supply chain and operations. I have a whole bunch of images of screws. I'm trying to figure out what's the ratio of defective screws, uh, based on similar types of technologies that I might have, uh, when I'm using a model to, to predict whether or not this is a cat or not again.
20:25
Okay, basic extracting information from these images somebody's gone through and checked these to see if they're defected manually. And then there's a follow-up study that's going to look at this data and see how well can the model predict whether or not what there's what it's seeing is actually a defective screw on it?
20:45
What is what is business analytics and how are they connected? Um, and what I want to jump into next is the four main levels that you can think of within business analytics. That sort of levels of sophistication to some extent in terms of how the organization is utilizing its data and using tools to extract information from that data to build decisions.
21:08
So, this is the type of chart that's been around for for a long, long time, uh, it. It has what, I think, our three, very distinct and important. A criteria to distinguish business analytics, starting with descriptive, then going to Predictive Analytics and then finally coming on with prescriptive. So descriptive is is discussed here as looking at past data to report and visualize things.
21:33
What has happened predictive data is building a model of some sort? Be connecting responses to to data that you have trying to make predictions in areas where you don't have data. Yet, that could be the future. It could be a different Market. It could be whatever. And that prescriptive is based on everything that we've done.
21:50
What should we now do? Okay and? When I think about a lot of companies, they're pretty good at the descriptive component, meaning they look at their data. They visualize it. They do summary statistics they use Tableau. And that's great. I think that's, that's a good first start. To really start to understand your data, I don't think you could ever really do a good job with either predictive or prescriptive unless you spent sufficient time doing the descriptive data analysis as well?
22:18
Unfortunately, there's a lot of companies that sort of stick or stop at the descriptive phase. And so I have a provocative statement here. Which is, if you're making business decisions based on Tableau. You're doing it wrong. I don't know if I heard anybody's feelings with that. If so, I apologize.
22:36
Uh, but there's just a lot more that you could do with data and just getting descriptive data eliminates the power that you can get from a model. Models can sometimes be complicated, but you can also specify something relatively straightforward. And that's allow you to make a prediction, but then a prediction by itself isn't all that useful, either, unless somebody's actually going to do something with it.
22:57
So, I've had this happen repeatedly in our Capstone projects where a company says, I just want to know if you guys can give me a better forecast. Okay, great! Now I've developed a model that has a better forecast, right, better accuracy? What's that going to change in your decision making?
23:12
Unless it changes something. There's not much value to it. But ultimately, the goal of all of this work should be. What do we do now? Right, what's the decision that we're going to make based on this?
23:26
Okay, so there's a question. Can you explain the difference between predictive and prescriptive? I hope I clarified that a little bit predictive would be something where you take a model and you make a prediction. Right, there could be about something that hasn't happened yet. Let's say I'm trying to make a prediction for what sales would look like in the next month or the next year.
23:41
Um, could also again be a prediction for an area. Let's say I have data from a few countries, and I'm trying to make a prediction for what sales might be in this other country that I haven't entered into in the past, and so that would be a prediction. But then, the prescript of it is now.
23:56
What should I do as a consequence, right? So, maybe the prediction shows that the demand in that country is going to be very low? And maybe the prescription is, you should not enter that market. Right, that's a possibility. Uh, it could also be we should enter that market, just not with this particular data or not.
24:13
Sorry, not for this particular product right with this particular marketing program. We need to figure out a more detailed way to go into this this new market, uh, that doesn't sort of align with what we've done in the past.
24:27
Uh, hey, David, so I just, uh, I hope I again was one of the people. If I didn't, I hope I didn't offend you. The blue is indeed a very powerful data visualization tool. It can be super helpful for people to understand their data and see what's going on.
24:40
Dashboards are great, especially if there's not too much in them. Especially our students. They like to see how many tables and graphs they can get into one single dashboard. Like, the more, the better it seems. But this the specific thing that I want to get at is? It is, it is not a decision support tool.
24:58
And a predicting model isn't isn't one, either in and of itself, but it allows you to do much more with your data than just sort of a. These are the markets and these. This is the demand at those different markets. Okay, so I'm happy to have additional discussion, but I want to go go through a few more things before we get back to this.
25:14
I've relocated with the distinction between the three. Okay, cool. Thank you for the nod. Appreciate that. Okay, so descript. Uh, again. I'm assuming every one of you has done some of this at some point in your life with a cell with Pablo with something like that, uh, we're generating plots.
25:32
We're summarizing data over time. We're looking at different groups, geographies, you name it, calculating averages, histograms min max. Again, this is sort of the area where I see. Excel Tableau power bi, you name it being really, really good. I tell my students one of the one of the quickest things that you can do to really, you know, help.
25:51
Your resume is learn about Tableau. You don't have to be like a super expert, but having some ability to use Tableau is going to be very, very valuable. Because people respond to visualizations usually very well, and it might lead to a lot of questions that, then you can start to research it more down.
26:09
Okay, has anybody seen this as a plots before?
26:17
So, this is the very famous. Quartet of plots, and what's interesting about them is that they have the same means correlations, standard deviations. Even though visually, of course, they look completely there. Hey, this is quartet. Uh, most statistics books have something like this in them somewhere. They say slightly more modern version of this.
26:40
Which is this? And now, notice what happens with the numbers on the right in the screen, right? So the means of X and Y stay the same? The standard deviation stay virtually the same. The correlation is virtually the same, but obviously, the data looks massively different, right? So, this is referred to as the data source.
26:56
And assure my students this as a way of sort of visualizing or sort of. Yeah, also visualizing how important visualizations are. There could be all kinds of really weird things happening in your data that you have no idea about. We could be building a model on the most rubbish data in the world.
27:09
The model might look great. You might have a great hour square or something like that or a great, and you see that at the end of the day, it is so. The first thing you should always do is visualize your data. Do plots those timesheets that sounds rather mundane, but it actually ends up being a big part of your of your work.
27:25
Really getting to to get a good feel for what the data is, how reliable is it, what it? What does it look like? What are the connections? Okay. Now connected to this, I want to set up our first poll. Now, I don't have control over the over the poles.
27:40
Can somebody set up start the poll for me?
27:49
Ah, there we go fantastic! Okay, so before you? Before you answer. You're a little secret. I hope you can still change your answer because I wanted to introduce it first. So, this is a a graph real world situation. Uh. Is a challenge in the Auto industry, which is making connections between whether or not somebody's seen an ad.
28:13
Let's say, you know, somebody goes to Google. They put in some Search terms. They see some ads and then connecting whether or not they saw an ad to whether or not they ended up purchasing a car. That's all. I've been rather rather challenging, and was that executive Retreat where somebody got up and said, hey, we fixed this.
28:29
We figured out a way to close the loop so we can now make connections between whether or not somebody was exposed to an ad, uh, let's say for a Honda or Ford doesn't matter right. And with that information, we can also see whether or not they ended up buying a car, and So based on this, we can evaluate what the effectiveness is of Google ads.
28:46
So, this is the data that you're seeing in this graph, which says, if somebody didn't see an ad, it's very for a car, uh, then there's very low chance that they would buy the car. This is our retailer ad. It goes up to three percent. Okay, so a manufacturer ad, let's say for Rondo or Toyota.
29:01
This goes up to five percent, and if they saw both a retailer and a manufacturer ad. The probability that they buy a car ends up being 14.
29:13
That's what I just described, including the graph. Does that demonstrate the Google ads? And so I would like you to fill out the poem, hoping everybody can see it. Go ahead and fill that out real quick.
29:35
Now, don't put anything in the chat yet. We'll get to consider your opinions about why one or the other in a second.
29:48
I don't see the pole. There's a lot of people filling in the poll.
29:58
I don't know exactly how this works. Somebody turns it on, and it magically appears.
30:13
Okay, once you've clicked then it, then it disappears. I don't know if there's a way to get it back up. Okay, I'll give you 10 more seconds. Professor nice. We should be able to share those full results as soon as everyone finishes, so just let us know when you want those shared.
30:32
Okay. All right.
30:40
Okay, so I'll end the poll. And I assume that means everybody will be able to see the poll results. Okay.
30:50
I always think so. I've asked this question from a couple of different artists in the past, and I always think that I that I set it up so that people think that they get suspicious. I think, well, why is he asking this question? Maybe the way he's phrasing, it means that it doesn't work or what.
31:04
So, I'm assuming some of you were thinking along those lines, uh, let me give you a little bit more of the backstory. So, again. They had closed the look. They were able to make a connection between whether or not somebody saw an app right for some car. Or for some retailer and whether or not they ended up purchasing right and the claim by the people that did this data work and analytics was that this indeed demonstrates the Google ads work.
31:28
Now, here's a question. Suppose that you go to Google. And you type in a bunch of sharps.
31:39
Let me rephrase this. Um, why do you think these people over here? I don't know if you can see my cursor. That they might not have seen any ads for cars when they were.
31:54
All right. Go ahead. Say something funny. Their phone heard. I'm talking about computerized cars. Okay. So, you've hit the nail on the head right there. Okay, so if you think about it, if you go to to Google, what are you going to see our parsley? Sorry, what are you going to see ads for most likely something that's related to what you're either searching on now or what you've searched in recently?
32:20
Okay, so. Does. Does Google assign these these conditions these these ad conditions? If you will like seeing an ad not seeing an ad, seeing a manufacturer had a retailer ad, do they assign this randomly? The answer, of course, to that would be no. That they would have been out of business a long time ago.
32:37
That's an interesting sort of research perspective, but you're not going to make any money now, right? So again, think about this. If you're in front of your your computer, you're typing into Google, uh, and you're doing some searches if you're not searching for anything related to cars. It's pretty unlikely that you're going to get an ad related to a car.
32:53
Right? If you, if you are searching on something related to, let's say, a dealership like, where's the nearest? Honda multi or something like that? Then, I might get some information, or maybe they have information about your ZIP code. Uh, if you search for a particular make maker model, you're more likely to see a manufacturer ad.
33:12
And if you're searching on both and in the enter, they know your location, then they're even more likely. To to provide you with both the B10 manufacturer. Okay, so, so the the idea here is, it looks like a graph like something you might get out of Tableau, and it looks for sure.
33:29
Like, there's a strong correlation between whether or not you saw an ad and whether or not you ended up buying a car, right? I think there's no doubt that there is a strong correlation between these two. The question, then, is what is causing that correlation? Is it because people saw ads and based on that they decided to buy a car?
33:44
Or is it because they expressed their interest in buying a car based on their Search terms. Then, Google said, hey, it looks like you're interested in buying a car. Let me show you an ad for that. Right, which is a completely different story. And to people that have had sort of some training, thinking about their data, thinking about causality.
34:02
In particular, they often get this really quickly, but most of the time I get something along these lines sort of between 30 to 40 to 50 percent of people say, yes, it works, and other people do not right. And so, they see a strong correlation. They think that that's evidence, but you have to think a little bit more deeply about it.
34:16
Like, what does the data actually tell me, and how is it generated, and if it's generated random, then yes, this is really good evidence, right? This would look like really good evidence of a connection between showing somebody an ad and whether or not they purchased. We don't often have data in a normal sort of business processes, whether it has been assigned, we do that in experiments, and there's lots of experiments happening in Industry all the time.
34:40
But if, unless it was randomly assigned, there could be all kinds of other factors that are driving this correlation that we may not be aware of. Hey, so, it, it doesn't say for sure that Google ads? Uh, sorry it doesn't. This doesn't prove that Google ads don't work. It just says that this data is not sufficiently clear for me to establish whether it does or doesn't.
35:01
Right, there's no way to disentangle that based on the data. Okay. Good first, Paul. All right, so. Again, this is one of the reasons why I am a little concerned about people using just Tableau and bar graphs like this, or Excel, and borrow graphs like this and say, hey, I see a connection.
35:22
That's great, but you haven't established that. That is a causal effect, and one thing that's a bit more challenging to do in a situation like Tableau, which is, I can't control for other things. Right. So, for example, if I had some measure about somebody's interested buying a car in advance, I could somehow build that into this.
35:41
And then I see, you know, whether you were exposed to an ad? Did that lead to an increase or decrease in your probability of purchasing a car. I'd be a lot more comfortable that this might indeed be a causal effect. But in a situation where I don't have any additional controls, and I just see whatever Google decides that they're going to do, and this is the outcome.
35:58
I don't have any controls. I don't have any experimental design. There's no way for me to establish whether one thing led to the other thing, there's no way to establish causality. All right. So, then the second level predict of analytics using data, you have to predict data you don't using statistical or machine learning models.
36:17
I kind of like this definition because there's a lot of data that we don't have, and maybe I can use a model to predict what the, what the, what the market will look like next month, or what this other Market or other set of customers that I haven't talked to yet, how mild they might respond?
36:30
And we're trying to sort of fill in the blanks. And there's a ton of different areas where you can apply these a lot of the things that I teach in relation to customer analytics. We're looking at things like purchase probabilities, response to some type of action, and add whatever it might be, but also in areas like supply chain, I'm trying to predict demand and also inventory levels.
36:49
I'm trying to predict the churn rate for customers, and what are factors that might drive that and what I'm what might be able to do about that failure of Parts in different Machinery. There's a huge number of areas where you can apply this type of modeling. Within industry and within the classes that I teach.
37:06
There's sort of three main buckets that I would classify the types of models into starting with something that I would label as a heuristic model. Has anybody heard or applied rfm analysis? And the rfm stand for recency frequency monetary dot. I'm looking to see if I see anything in the chat.
37:26
I'm just seeing some people. No, okay, all right now. You may have heard of the variables, though. All right. So, what does recency stand for? When's the last time that a customer in your database purchased something? There might not be a field sort of by default for that in your in your transactional database, but somebody can calculate this pretty easily, right?
37:48
There's a customer that's been with us for a while. When's the last time that they purchased? Uh, how often have they purchased that's frequency? A monetary value is, how much have they spent? And it turns out these things are pretty good predictors of whether or not somebody will buy again.
38:05
So, do you think that if I had a data set or I have information about resources? Do you think that as a recency increases? The people would be more or less likely to buy again. Right? So, so a positive effect if you want to think about a regression context if recency increases?
38:22
Does that make somebody more or less likely to buy again?
38:29
Okay. So, so this is a bit of a tricky question. I made it made it a little tricky by, uh, on purpose. So, what is recent so recency is? How long has it ago since you last purchased? So, a low number means. It's very recent, right? It's only a few days ago or a few weeks ago.
38:44
Tops, right? So, as recency increases, what does that mean? It means that it's longer or longer ago since you last purchased from us. And so if somebody hasn't purchased from us in a year or two years, How likely is it that they're going to purchase again? No, not very likely.
39:00
Right. So, as recency increases, it becomes less likely that somebody's going to participate. Okay. So again, even though you might not have used this particular model, it's not really a model. But this is something that many, many companies have used. They've looked at information on recency, frequency, and monetary, and based on that decided who they might Target.
39:17
We're going to Target people that are more likely to buy again, and so that's a that's not a not a economic model that's not a consumer Behavior model. This is something that's sort of. Developed over time is a rule of thumb, and that's worked reasonably well. Our analyst driven models right, and these models require a lot of domains expertise.
39:37
So linear, Russian logistic aggression, or assuming most of you had probably had a class on statistics at some point in the past, but you were introduced to regression and some form or other. Really like these models. They're hard to build. They're not hard to get software to calculate, but they're hard to build, and they're hard to build because they require patience.
39:57
And they require domain expertise and by domain expertise. I mean, I have a good sense based on the market that I'm in the customers that I'm working with on what would be the relevant variables to put into that model, right? What would be a good response variable. What would be a good, explanatory field, right?
40:12
And how do I organize those? When my students use something called stepwise regression, whose word of stepwise regression before? Quite a few of you. I get very upset with it. And the reason is separate. Regression works really well with linear regression in terms of just sort of getting it to run and coming up with a model, but most of the time, you haven't spent any time thinking about exactly what that model then looks like, right, and how it's organized and why these variables are in the model and why others aren't?
40:43
So, basically what you've done is, you could estimate a linear regression again with your domain expertise, trying to figure out what the variables are how they should be organized, do their, and Transformations that are needed. Like, really using your expertise to figure out what that model would be. But if you've used something like stepwise regression, you've basically gone into the domain of what I would call the data-driven models or machine learning.
41:05
Right, and those don't necessarily need much Theory or domain expertise, or at least not sort of business domain expertise. They're very focused on what the data says, right? So they're sort of like that. Let the data speak type of models, and so this would include things like decision trees, gradient, boosted trees, random Forest neural networks.
41:21
Those types of minds. Right. And for those? To some extent, you push the data at that model. You let it churn away. You maybe give it some tuning things to try out, and then you just see what the prediction is. And they can end up predicting really, really well.
41:37
But the model itself is often a bit of a black box. Whereas these linear and logistic regressions that are built using domain expertise, I'm really trying to bring. Let's say my understanding of consumer Behavior or economics for my market and the market Dynamic that I understand into that model into building that model so that I can start to have some interpretation and some understanding of.
41:56
How are these different levers that are in my model connected with an outcome?
42:04
Now, all of these have their place, and in fact, what I, what? I love students to do is start with something super simple. That could be something like a heuristic model, maybe then start building out a linear regression model. That's not sort of a lot of technical complexity there, but I real need to develop your domain expertise, and only after you've done that start to figure out and think about whether or not data-driven models might be useful.
42:26
Unfortunately, especially when our students start the very first thing they often want to do is go estimate the most complicated model they can get to run. And then you ask him, what does it say, I don't know, but it predicts pretty well? Okay, yeah, you've got something important out of it.
42:40
Yes, the prediction is important, but let's take a step back before we start to make any decisions based on this. Okay, sometimes as students, even with a Capstone project, they get started. They get very enthusiastic, and they say on some some business problem, like predicting the probability somebody's going to buy, they have an AUC of like 0.99.
42:57
Like, okay, you did something wrong. It's almost impossible for you to predict something that well, right, in a domain where humans are involved. It's, it's. It's almost impossible. Uh, but they get very enthusiastic, and they don't believe me initially. And then we start sort of digging through the data and trying to understand where the problem coming from.
43:13
And then basically taking something like, I don't know. The quantity that somebody purchased as they're trying to predict whether or not somebody purchases something, right? So the purchasing something is a yes, no, and the quantity is, how much did they purchase? And then, if you use quantity of purchase as a predictor, but you can't have a non-zero quantity unless you actually purchase something, right?
43:32
So, they found some variable in their data. That seems to be really highly connected to the response variable. It's sort of like saying, I'm doing a prediction of the stock market, but I'm taking in news from tomorrow and using that in my model today to try to predict what tomorrow's financial results will be.
43:51
Anyway. So, there's a lot of different juice into space, and I'll just talk about some of the things more specifically related to that, uh, that help you. Do a good job in this space. And so this is going to be my second poll question. Which is related to correlation.
44:07
I'll set it up a little bit better after I've asked the poll. I want you to look at this, I hope you can see the colors reasonably well, so they're sort of a blue greenish, uh, set of dots that are connected, uh, to the sails in region two on the hardhold access price and on the vertical axis is sales quantity of sales.
44:25
So the blue greenish dots are the sales in region two and how they vary with differences in price and the red red. Pinkish color is region one again. Variation in price and how that's connected to the quantity of sales. Question for you, and this is the poll. If we can start that that up, please is which of these regions or which of these pages of data shows a stronger correlation between price and sales?
44:52
He's got their price with sales in region one, or is it price and sales in Region 2?
45:35
Okay, give me 10 more seconds.
45:52
Okay.
45:56
Okay, let me share their results. So, 55% of people said the sales price relationship in region 1. Exhibits stronger correlation. And the rating, 45% said there's a stronger correlation between price and sales in Region 2. Can somebody who said this correlation was stronger with price and sales in Region 2?
46:24
Uh, he would be willing to unmute and explain why you said that I'd be. Yeah, it's smaller. Very good that is indeed correct. Um, yeah, and then all of them are just tighter across the whole screen. That's absolutely true. And so somebody that said region one, why did you say the the correlation was stronger for region one versus price?
46:50
I said the what, um, I said, region one, because it seemed like it was more elastic, like the, as the price increased. There was more of a change in the sales. Yes, now. You're absolutely right about that. Now, unfortunately, I was, I have to. I have to come clean here.
47:08
I wasn't. I wasn't quite fair with you, uh, because what's actually the case? Is that the correlation in both of these data sets is exactly the same? And I didn't give you that option, so I sort of forced you into one or the other. Uh, and and how do I?
47:22
The reason that I know and why. You can see that the correlation is exactly the same because they simulated them to be exactly the same. You're doing, you're indeed correct. And, and seeing that the standard deviation is a little bit less in region two, so that sort of goes with it.
47:35
You can't have the correlations be exactly the same and have the same variation. In fact, I have a graph. I'm not sure if I have it in this deck where you can see region three, where almost looks like the line is flat. There's almost no variation, but the correlation again is is minus 0.97.
47:53
So? Let me show you as a reminder what correlation tells us. Correlation tells us something about the consistency of co-movement between data. So if one thing goes up, does the other thing go up as well? And if one thing goes up and the other thing goes down, that means we're going to have a negative curlation.
48:10
That doesn't say anything about the slope. Right, it's sort of intuitive that there would be a connection between those two, but that is. That is not necessarily the case, so if I want to understand slope, what do I need? I need a different tool for that. I, I need a regression.
48:25
Progression is the thing that's designed to give me information about a slope. Correlation only tells me something about the consistency of. Again, this is just useful as you're looking at your data. You wouldn't want to make a decision about which Market you would want to be in just based on sort of correlations.
48:42
You probably want to look at at actual slopes and regressions. If I look at this regression here on the left, that is, the effect of a price change on sales. And then, for the other Market, which is sales two. I have a similar regression. But I see a number that is closer to zero.
49:06
So, if you had to make a choice all else equal? Which Market would you rather be in? This is very much of an economics type question. So, what did you put in the chat? Which Market would you rather be in? Would you rather be in Market one, or would you rather be in Market two?
49:20
Region, one or region two. There is no Market three. Okay. Very good. And and why Market two?
49:33
Get marketed with. Yeah, it's just like, even though you increase the price followed by, you still get a profit. Yes, so, so demand is less sensitive, so that should imply according to economic theory that you should be able to charge a higher price. Ah, so maybe you're more differentiated or you have high quality or whatever else that might be causing this.
49:51
But there's less of a response to changes in price so I can increase price more in charge. A higher price that's sort of what the model suggests.
50:03
Right now, an additional com component here. If I, if I look at this particular regression? Uh, I'm making a prescription here, right? I'm saying I've built a model. I've looked at the data I visualized it. I created some graph about it. I built the model. I've done the predictive modeling, and now I'm making a decision right.
50:20
A prescription or recommendation prescription about which Market would I rather be? So, that's sort of that. Third, that third area of business engines, and so what that requires, of course, is some assumption about causality. Is it indeed the case that when I change the price that that was the thing that led to a change in demand?
50:41
I could of course also be the other way around. Maybe price doesn't do anything but?
50:47
To adjust after the fact by changing my crisis. Maybe less likely, but it's technically possible, right? So I'm trying to make some type of a an assessment of causality here. So this comes back to this graph that we just talked about earlier. Which is, if I look at this, and I say, hey, it looks like Google ads work.
51:03
Maybe we should do more of this? I'm making a prescription. I'm looking at my data and and I've got a just just a graph here. I'm making a prescription, which is, we should do more of that, right? It's worthwhile to invest in Google ads again that requires some understanding of causality.
51:18
In this particular case, is it because we showed these people ads that they ended up purchasing it for right, and that requires quite a bit of thinking and understanding in particular about what I have up here, which is the data generating process. So, how did that data come? What was the process to wish it was generated.
51:35
It was randomly generated. I would be perfectly comfortable here, saying yes. Google ads work, because in this case, that would mean some people were randomly assigned to not see any ads. Some people randomly assigned to see some some ads, or maybe some combination of ads, and if I then see this pattern, I'm perfectly comfortable making a call.
51:50
Okay.
52:09
Those are not the most sophisticated models, right? Those are again. What I would consider the the models that require domain expertise, the sort of analyst driven models, and it's what I always recommend to anybody that I talk to when they're starting to get into data. Data, and analytics is start with the simplest model.
52:23
You know how to run, not the most complicated? Complicated one down the line, but always start with something that that sort of you could almost calculate just straightforwardly in Excel. And so most companies that are good at using analytics and data they they use these tools, but then they might also use additional more, more fancy models as well, right?
52:42
And so, decision trees random Forest graded boosted trees, neural networks of various types. Advanced machine learning and deep learning? Absolutely right. Um. I'm curious here, who has estimated a neural network. Can you put in a chat? Just say, yes, right now.
53:13
Okay, so definitely a few. Yes, it is, but more of you haven't have a no here. Um, who who has used?
53:28
Okay now, um? Who has used chat CPT before or one of its competitors? If you've used that, you've used a neural network. Right. So, the underlying technology that drives chat TPT Gemini Claude, you name it, are often referred to as deep learning models. There's a bunch of other built-in whistles to it, but that's sort of the underlying underlying architecture Transformers.
53:52
That's a deep learning model, and deep learning is is a forum of a neural network, so I would argue I'm assuming that pretty much. Everybody has used this by now in some way, shape or form. And so, what I'm going to walk you through is just an example of what a neural network might look like.
54:07
In my experience, it's very unlikely that you're going to need anything. Like, for those of you that know more about it that you would need anything like a deep learning model. Business brand. In most instances, simpler models do at least as well, if not been. So, this is an equation that I'm assuming again.
54:28
Most of you have seen in in some way, shape, or form in a statistics class at some point. There's some response variable that I'm trying to predict, let's say, sales, and there's some explanatory variables X1 and X2. And I just want to show you a depiction. Okay. Of what a neural network version of that might look like, and so it's almost the same model just depicted a little differently.
54:53
And usually when I see all my students this for the first time. They're like, wait, what is this? And how is that connected to a regression? So, if you think about a regression, a regular linear regression, I'd like people to think about a a spreadsheet. I think about an Excel file.
55:06
And that Excel file is going to have a bunch of columns. Right now, those columns might be either something you're trying to predict or something you use to predict something else. Let's say I've got a column with sales, and I've got a column with advertising. We've got a column with prices now.
55:19
What are the X1 and X2 that you see here? Those are just Columns of explanatory variables within my within my data set. All right, so X1 might again be. Price X2 might be advertising, that's. And what's the why? That is going to be, let's say myself, right. I'm trying to use variation in price and advertising to see how much sales have changed over time.
55:41
That's basically what this is. Now, what do I have happening here? So these B1 here is almost the same thing as the B1 and the regression equation. The B2 that you see in the graph is almost the same as the B2 in the regression equation, right? The b0? Is often for case a bias term in neural networks, but that's very similar to an intercept.
56:02
So, this graphical depiction is very, very similar to to a simple regression model. In fact, we produce almost exactly the same results. Now, if that were the case, why would anybody ever use neural networks like the simplest form really isn't all that interesting? What becomes interesting is, if we make them bigger?
56:19
And this, by the way, is still an absolutely tiny, tiny neural network. And so, here I have X1, X2, XP. I just added another explanatory variable. There is what's called a hidden layer and a hidden layer. Just means that's not a column that's in your data set. Right. It's not something that I can point to and say, hey, that's the data.
56:36
Those are weighted combinations of the data that is actually in your data set, right? So X1, x2h3, price, advertising, promotions, or something like that and then some weighted combination. That's what this stuff here is in what's called a hidden layer. And then we try to use all of those sort of observed data and the hidden hidden nodes to try to predict some outcome variable, which might be y here, which could be sales or the probability of somebody buying or whatever else.
57:02
So, it looks a little bit more intimidating, but again, it has very, very strong connections to a regression and. What you, what you might think of as a neural network, is a highly non-linear progression model. Right now. Why might you, might you want non-linearities because? Connections between, let's say, price and sales are probably not completely linear, uh, there might be all kinds of interaction effects you've probably heard of those at some point, and if I if I charge a higher price, but I support it with lots of advertising.
57:31
Maybe the effect of of the price changes is going to be as big as it otherwise. That would be an example of an interaction. Now, why am I talking about these these models here? We're obviously not going to estimate any of these because that that takes a little bit little while to figure out and some relevant software just to give you a sense of where these have been used in the past, and again, this is a sort of the smaller brother or sister of what you could consider deep learning.
57:53
And deep learning is a type of technology that you want to use. Uh, to estimate something like a chat GBT right or a gbt4 4.1 or something like that? And so this has been used in a wide variety of areas. Some of the first application areas were related to recognizing handwritten digits, so that's been around for quite some time.
58:10
Autonomous driving. There's also where deep learning is very widely used. Facial recognition. And so there's lots and lots of areas where it's very difficult for us as humans to build a model ourselves. But these neural networks are deep. Learning models are really, really good at making connections in a variety of different ways.
58:28
With data, that would be almost impossible for us as humans to sort of. Do a good job at building a model. And so. Areas related to that that you might have seen this image at some point in the not, too, not not too distant past, uh, Dali two, uh, I was very impressed at that point with all the, uh, the fact that you could basically with a text prompt, generate an image of a rabbit detective, sitting on a park bench, reading a newspaper in a Victorian setting.
58:54
But obviously these models have gotten incredibly fast. Sorry, incredibly better over time, and so there's a huge number of companies that are in this space now. There's a huge amount of development in this in this area. I'm going to skip this for a second because I want to show you.
59:13
I want to talk a little bit more because we're we don't have a ton of time here. I want to talk a little bit more about. Uh, let's see skip lines. I want a little bit talk about about prescriptive analytics and then a little bit about generative AI more more thoroughly.
59:28
So, if you think about those three levels that we talked about descriptive, predictive, and prescriptive? You can't do a good job with prescriptive unless you do the other two well, but I'd say unless you have a prescription. There's a high likelihood that you've wasted your time. The fact that I got a better prediction of some outcome variable is great, but if somebody can act on that, somebody going to change something in in a business process based on this improved ability to forecast something.
59:55
If not, then it's just sort of a novelty exercise or an intro interesting, potentially intellectual exercise, but you haven't really done anything with. And so, here I'm thinking about, you know, deciding on maintenance schedules, uh, who should we send a sales rep out to talk to? We don't have enough capacity to have them talk to everybody.
01:00:12
So, who are we going to Target most? We can use models to help us figure that out, uh, proactive churn intervention. Like, there's a whole bunch of people that might believe, but we're going to reach out to. They want to make sure that we keep as a potential customer, uh, setting prices, allocation of assets.
01:00:28
You name just lots of different areas in this in this space, and we could do the prescription in a very simple way. If demand forecast for this particular Market is about some threshold, we're going to enter, right? That's a free scripture. We thought about what we're going to do with the predictions we've decided on action.
01:00:44
Given the amount of demand, let's say, that's forecast.
01:00:52
Tools, linear programming, non-linear programming your name. And I just wanted to give you this as an example that I use used in one of my classes. This is an example of a decision tree. Um, where I have information about whether or not somebody bought an art history or Florence book.
01:01:10
And I have additional information about when did they last purchase? What types of books did they purchase and so on? And so, what this decision tree has done is, it's taken all the information about whether or not somebody purchased an art book. Can you see my cursor, by the way?
01:01:24
Okay, good. Whether or not somebody's purchasing art book and then try to sort of split out the data to get that sense of. Can I find groups of people that are more sort of homogeneous in their willingness to buy or not buy exercise, splits the data sort of filtering the data as we go?
01:01:38
And so, what do I see here? I see if somebody purchased the artist to Florence, they're also likely to have purchased one or more art books in the past. And in fact, if they've purchased two or more art books in the past, the probability that they would buy this Archeristic Florence book jumps up to almost 34.
01:01:56
All right, so this is a machine learning model. There wasn't any sort of sort of domain expertise necessarily involved in building this sort of just statistical machine learning algorithms coming up with what's the best possible split of the data. But if you look at this, the interpretation is pretty simple.
01:02:11
Right, I might have some threshold. Like, if the probability isn't at least above, you know five percent or ten percent whatever it might be. I'm not going to Target these people. For example, if I'm looking at whether or not somebody's going to buy an art book, and this person hasn't bought from me.
01:02:25
In, let's say, a month or a month or less. Um. In fact, this is people that have last purchased more than more 10 months or more ago. The response rate is only going to be about three percent. So I can just look at my date until through the date and see how many people that haven't purchased from us in a while, okay, I'm not going to Target them to this outfit for a book.
01:02:45
I think there's the probability of them. Buying is just too low. Uh, and so I can just look at this graph and just highlight those that have the highest probability and say, oh, this is instead of people we're going to Target for some new offer, right within our article percent.
01:02:57
So, sometimes the prescription from a prediction can be very straightforward and just look at this graph and say we're sure that people that are most likely to want to Target, but it could be much more complicated in the sense where I would use simulation or linear programming, or something like that to figure out what an optimal decision like?
01:03:14
Okay, so, so those are the standard components that have been around for a long, long time, right? This sort of the the building blocks if you will of what business analytics is all about. It's the descriptive, the predictive, and the prescript, right? But, what's become more interesting more recently?
01:03:29
Is this idea of artificial intelligence, and how does that fit into all of this? Artificial intelligence as a discipline of study has been around for many decades. Uh, but what's what's less clear sort of? How do we now try to integrate that into our business processes? At some point we might want to at some point, we might not.
01:03:48
And so this is the the last poll I want to do with you guys. The question here is? A statement. Uh, the the wave of generative AI tools, like chat, gbt Gemini and Claude VO, and so on, will transform Industries. Their impact will be similarly disruptive to the invention of electricity or the internet.
01:04:07
And so, what I'd like you to do is just think about that particular statement and indicate to what extent you agree or disagree with that state.
01:04:56
Okay, I'll give you 10 more seconds.
01:05:14
Okay. All right, here are the results.
01:05:23
Okay. So? I, I've done this poll quite a bit. With different groups of people, and most of the time, it's the strong degree in degree buckets that are that are, by far, by far the biggest. Uh, there's always at least one or two people that are in the neither agreeing or disagree.
01:05:43
And sometimes, there's some people in the disagree bucket. The, the disagree, really interesting. So, I'd love to hear if somebody wanted to unmute and share their their disagree state. Uh, please go ahead Santi. Hey, how's it going? I think that AI is not going to be as disruptive as electricity, um.
01:06:07
Because it's a medium of communication. Um. And it uses a tall electricity. I, I just, I just don't think it's going to be as. Transformative electricity, that's a pretty big, uh, pretty big statement. Going from steam powered engines to electric trains is a big leap, and I don't know if KR is going to be that late.
01:06:28
Thank you. Okay, okay. So do you see it as being a big leap, or just, you think it just. It might be big, but not quite, as big as these other Technologies. Going to be disruptive. Uh, time will tell. Uh, yeah, time will tell. I think it's a little early to see how disruptive it's going to be, but it definitely has potential to do some.
01:06:52
Do quite a bit of stuff, but I just don't see it being as transformative as electricity. Okay, all right, I'm I'm completely okay with that. So, so the the obviously the, the final judge of that is at this point, if I'm a professor, oh, please, please go ahead? So I even though I voted a strongly agree.
01:07:10
I have read a lot about this side, the ones that strongly disagree, and one of their biggest factors is because this technology has been here. Let's say, two to three years, you know, outing the market, but it hasn't done based on data. Uh, it hasn't reached expectations in the practicality of businesses on, like the results on when it's implemented in businesses and the increase of output based on what they expected.
01:07:37
So, because of that, they think that it was too much, uh, fuzz over what it really would be, but I do think that we are just learning how to use it, and so once everybody, it's well implemented in the industry, and people know how to use it because even knowing how to askgbt is, it's a specialty on its own.
01:07:57
Once we get there, that's when we're gonna see that output up exponentially increase. Thank you. Okay, very good, very good, um?
01:08:22
Okay, all right, um, Shanti, go ahead. I mean, okay, sure, I mean, we don't right now. As the previous, uh? Student mentioned that it's only been out a couple years. We don't know what they've created in in the Laboratories on the, you know, their offices, uh, it may not be.
01:08:40
It may not be released right now. This products, and uh compounds, they've used it for to create are pretty. Might be, might be sitting, might be, you know, incredible. Like, uh, cures for cancer, uh, Gene editing, blah, blah, uh, but they just haven't put on the market, because, uh, you know, they'll slowly trickle out the, uh, you know, the?
01:09:03
What's out there? I don't know if they immediately put things in Market. You have to have, like, in medicine, you know, you have like a long pipeline for FDA approval, so? It's, it's done a lot of crazy. A lot of really amazing stuff. I, I still have some reservations on.
01:09:17
It's power yet. Yes, I mean, there's no, so I think one of the good. There's a there's a lot of Regulation related to, let's say, new drugs, right? And so, that's there. There's there's a lot of, uh, a lot of wisdom in that, maybe sometimes it could be a bit quicker.
01:09:30
I don't know, I'm not an expert in that space, but it's definitely useful to have people check sums. I don't know if you've seen some of the things that grog seems to be saying on X at the moment. Would be better if we had some regulations or some some quality checks.
01:09:42
Now, that's it! Do I have any hope that the government has a good sort of technical ability to actually evaluate these models. I don't think that's likely to be true either. But okay, yes, miles. Go ahead. And the reason why I said strongly dream is personally look where the internet was at again 25 years ago.
01:10:03
There was no social media. Um, that was a big one. There was no Web 2.0 or anything else like that, and then within 10 years of that social media starting to come out, but 3.0. I started coming out and everything else like that. The murderverse and everything else like that.
01:10:19
So if you were to look 25 years in the future with all the research, and yeah, developed me and breakthroughs, of course. AI is ultimately going to get stronger. It's just we're in, like the early stages of AI. And then, like I was reading. Would be like the largest transfer of wealth coming up.
01:10:40
Like so far in human history, so that just goes to explain, like, how really disruptive is, you're really going to be, For Better, or For Worse, because it still has to be regulated. It still has to be built so as to be developed and everything else like that. But again, it takes time for us to get to where we're, let's say, we're at.
01:10:59
With iPhone, we had the iPhone one or the iPhone 16, so you have to let them develop all the features and everything else like that over a amount of time. Because you're, we're not going to go from dive from one to the iPhone 12 by tomorrow. It's going to take time for us to eventually to get there and everything else like that, so that's mine.
01:11:19
Thank you for this. No, I remember when I interviewed, uh?
01:11:30
Form everything, and they thought that it would happen, like, within a year, like, really. Like within a year. Uh, so I was a little skeptical about that, um. I just saw something the interesting year of Vincent, uh, so I was in college at UCSD in the early 90s. And I was getting mail on stcc13.ucsc.edu, so remember?
01:11:52
By a pine. Probably, nobody here knows what that is, but that was a green screen, full text based program, and at the time that was the most Buzzy word you could possibly say internet internet people didn't even know what it was. Somebody said earlier, I think it was Shanti that you know both students that, you know, gbt hasn't really only been out for a few years.
01:12:12
I think that's a slight misunderstanding. Artificial artificial generative intelligence has been out for quite a bit, and when you think about models you think about, uh, recommendation engines those have been in place in the industry for quite some time. It's just less access to end users to be able to manipulate those models, but they've been in place for a while.
01:12:34
So, the internet was actually born. In 1969, the real internet, uh so, between 69 and 99, when the bubble burst. And you know, we got starting exactly and everything else that time us 30 years. So, if you think about that today. Said strongly agree because I mean, look at the question electricity internet AI.
01:12:53
Ai is already solving problems that we can't even imagine. It was a two-year-old with cancer who is now completely cancer degree. They had written this this patient off completely and using Ai and modeling to have gene therapy and prescriptive. Radiation. Zero cancer in the body today, so? Already completely.
01:13:14
Dwarfing let the internet could possibly give us the internet, like electricity, is a foundation that's the way I look at it. The next big thing, on top of that Foundation, is generative intelligence. This may be made me think for myself. When was the first time that I heard about the internet?
01:13:29
I don't know that this was a term that it was used more broadly at the time, but at that point they were calling it the electronic highway, and I just sort of heard this in passing for the first time, and I came home to my wife and I said, I think they're going to be putting electrical stuff in highways or something.
01:13:44
I'm not sure that they sound very excited about it. Going to be at that time. Alan, go ahead. Yeah, I was. I had a question Professor on the response of that you've seen over time. On this, does it? Is it kind of like hit a certain demographic like, okay.
01:14:04
For those of us who are chronologically mature. In other words, we're older all right. I mean, for me, I'm like, I take a monster. Neither a green nor disagree or not that I disagree, but it's just I'm kind of nebulous, and I wondered if if for the younger people that have been immersed in in, in the internet and and the the electric Highway is, I mean, are they more?
01:14:32
To strongly agree or agree. I'd be interested, I haven't. I haven't done this with undergraduate students yet. But most audiences that I get. I sort of. It's about 5050 between strongly agree and agree with sort of a minority and some of the other markets, right? There's very few people that are in the strongly disagree.
01:14:53
But there are definitely some people that are saying, you know, what? Those other two were such big sort of interventions that I don't see it being that big yet. Okay, I'm I'm personally, very big on the strongly agree part and and strongly agree, set. Okay, um, if you think about a lot of what our life, especially for people in the in the younger Generations, are that in, in sort of Technology, technology runs on on math, on statistics, and on data.
01:15:20
The tools that are coming out. Are incredibly powerful. They can increase productivity by an enormous amount, and you may have heard some of the some of the CEOs or different companies talking about the impact it would have on on white collar employment. Already having that impact already starting to have that impact?
01:15:41
Yes. So, Sam Alman said that in he said something about two decades that all basically all activities that fall under white collar work could be done better and faster with generubia. Maybe 20 years? I think 10 years is, is very likely. It's that fast. So, we think about what most of us do all day, right, is.
01:16:04
We're in front of a computer of some sort and and we're doing our work. The amount of things that it can already do some of the Technologies right now. You have to pay a little bit more for that would cut my work. But other. Don't tell my boss. But.
01:16:21
Just the sort of the efficiency gains that are there and the impact that's already starting to have in large companies where they're anticipating that sort of the entry level. As analyst, or or a finance person. They might need, you know, 50 percent of what they, what they needed, probably crap previously.
01:16:41
I don't know how that's all going to pan out. I'm definitely concerned about it, but the thing that I try to do with my students is to prepare them as best as possible for a future that is going to be very much AI supported. Or AI assistant. Right. So, everybody's going to be using these these tools all the time, and in fact, I think that the chat interface that we're used to is probably the clunkiest that we're ever going to have.
01:17:07
You can sort of type stuff in. You can potentially copy and paste stuff in, but it's not really integrated in any meaningfully. Nice way with all the tools that we use in a day-to-day basis. And so, I think there's going to be a lot of interesting things related to how we engage with, let's say, the internet.
01:17:24
Has anybody seen computer use and what's the other one called that open AI has? Our computer use is the one that that, uh, arthropic is using and basically what that does is it takes a screenshot of your screen and then navigates the mouse to where you would usually click.
01:17:41
And then it puts text in. And like, that's just the stupidest interface that I can imagine, because that works how we work. But that doesn't mean that that's a smart way or an efficient way for a computer to work, right? It's sort of a. A preliminary stage until we get to something where maybe our agents are talking to other agents across across the web.
01:18:02
But an interface where it's basically replicating, how we do our work, is probably incredibly inefficient. In fact, it's not going to be that long before we're the bottleneck in sort of the, the getting the work done because we have to review everything. Right? Anyway, I wanted to talk about that miles.
01:18:17
Go ahead. Do you have another comment and I'll jump further in? Like what she said about white collar work and everything I was like? But like the one thing that, hey, I would never be able to do is have human creativity and everything else like that and be able to actually understand, like human experience.
01:18:33
Because, like, right now, there's nothing like human experience. Been anywhere. That's found on this Earth and everything else like that, like, yeah, you might be able to think like a human, but actually, I have human create creativity. I don't think you would ever like, have that, because therefore it's not living.
01:18:53
Yes, you could argue, like the program versus program me and everything else like that, like, yes, that's most definitely going to be ethics that will come with AI and everything else like that. I do feel like we might potentially get a version of Westworld somewhere in the future. One big AI will never be able to do is take away.
01:19:22
Experience, no matter how advanced it is. So, so my wife works with kids with autism. And so I currently can't imagine. Um, that that's something that that an artificial intelligence could do in the way that she does it. Uh, just the ability to make a connection with human that has a really hard time making a connection.
01:19:44
I just don't see that yet. Could that happen with robotics and gender VI? And you know, at some point, maybe, but those are the sort of areas where I see where something that really requires a strong human connection that that's not something that this is good at yet. That said, uh, there's been studies where they've had communication between doctors and patients, and sometimes the communication was actually coming from the doctor, and sometimes it was coming from from an AI and then the.
01:20:18
Ah, I'm blanking on the word, but the way it was perceived in terms of how caring the message was was much higher for the one that was generated by generative, yet I think part of the reason could could be doctors like, hey, I have like 30 seconds to send this person something you're fine, or you're not fine.
01:20:33
And that's it, right? That's maybe what they type, whereas a generative guy might have a nicely worded message, saying, hey, there's an issue potentially you want to look at that with you more, blah blah, blah, where the doctor just doesn't have the time, or maybe the ability in some cases to do this as well.
01:20:46
Okay, all right, I want to jump in a little bit. This is, this is a really interesting area with lots of positives, but also potentially lots and lots of negatives. Um, and I just read in the chat, which I wasn't aware of that was part of the the big, beautiful Bill that said no regulation on AI for 10 years.
01:21:13
Going to be the best, but if it is true that a large chunk of white collar work will be taken over by generative AI in the next 10 years. There needs to be some thought about what are we going to do then? And how is Society going to function?
01:21:31
Uh, so. Anyway. Okay, so I didn't quite come up with this this, uh, this question, all on my own. I adapted it a little bit from somebody that a lot of you are probably familiar with, maybe not the name, uh, but you've probably heard of Coursera. And he was one of the co-founders of that.
01:21:46
This is Andrew ning. He has been in a wide variety of very prominent roles across the tech industry, uh, I think he now mainly does Consulting. And I'm assuming makes an absolute boatload of money, both load of money from a variety of different companies. Uh, so AI is the new electricity.
01:22:01
100 years ago, electricity transformed every industry. AI is bringing an equally big transformation, and I fully expect that to come to pass, uh, it is absolutely the case that AI is not intelligent at this point. I'm assuming you, you realize that from your interactions, it sometimes does and says, the absolutely, stupidest things.
01:22:19
There is no sort of intelligence in the way that you traditionally think of human intelligence in generator VI at this point. Matter for whether or not it's useful. It doesn't seem to. It seems that it is perfectly capable of doing lots and lots of things that we would otherwise have to do with our human intelligence.
01:22:37
I still remember this was in, I think early, maybe mid 2023, was in a meeting here at UCSD and a prominent person at UCSD said, hey, I put my name in it, and it got my affiliation rock. And based on that result, she basically said, oh, this technology is worthless.
01:22:54
So, yes, it does make all kinds of mistakes. It does weird stuff, but if you have the patience, the ability to guide it. It is absolutely amazing. The amount of work you can get it to do. It is really, really impressive. Requires an investment of time and energy to try to figure out how it thinks.
01:23:11
And how it how it responds. In some ways, it's interestingly similar in the way it makes mistakes than what a human does. But in many areas, it's at least as competent as the most competent humans. And that's really interesting and potentially also a bit concerned and for for our employment.
01:23:27
I think it's going to impact education a lot as well. I'm closer to retirement than the start of my career, so I'm not sure how much I will be impacted, but. I think there's going to be massive transformations in a wide variety of areas, and so I think the best thing that you can do for yourself is get licenses and buy licenses.
01:23:45
I mean, multiple, uh, for two different tools. Explore and see how it connects to your work. This doesn't sound very nice. Maybe, but you're better off figuring out how you can automate parts of your work, then wait until somebody else figures it out for you. So, a really interesting quote from somebody a little while ago that works in the it space that was very relevant for them, but I think is more relevant for almost everybody here today.
01:24:06
Which is, you are what you don't automate. Right, so the stuff you automate? Great, something else is going to mostly take care of that. It'll leave you for the other things, so figure out what those things are that might be automatable. Figure out how to think about this figure out how to potentially do some of this, and some of you may have heard of vibe coding.
01:24:25
It's a very dangerous exercise could lead to all kinds of interesting problems. All of us here. Even if you've never touched the line, a line of code or written a line of code, you can create some interesting things that could be very useful in a small on the small scale on your own computer to help you do some things.
01:24:43
All right. I don't know if I when I shared if I shared the audio. Can you hear the audio that you that just came up? No, that's a bummer. Let me see if I can fix that real quick. I have to stop shifted. Yeah. I might have to stop share for a second.
01:25:24
Okay, that's too bad. I, I haven't set the share sound, but it doesn't seem to be coming across. Uh, which is unfortunate. I'm assuming most of you have seen something that was produced by Zora, which is which is a? A model out of open AI, a VO3 just came out from Google, and there's already models that seem to be beating that.
01:25:42
I don't think you can hear the audio, which is unfortunate, uh, but this is basically just a text prompt that says, hey, there's a little paper boat that's floating in the water. The quality of the video is is increasing very rapidly. It is sort of lay things, but to give you a sense of what this can do.
01:26:04
Let me stop this one. So, you can't. It's really important to you. You can't see the hear the audio. Uh, this is all completely AI generate. And so there's audio people are talking to having conversations. Uh, this is for a non-existent car show, so they're interviewing people and talking to them.
01:26:21
I'll just show you a little bit of a clip from it.
01:26:34
So? Again, really unfortunate. You can't hear the audio, um, but I saw this this, uh, this comment on on Reddit when it just came out. Somebody says my grandma isn't ready for this. I don't know if that's all. And so the, if you really look at it with a critical eye, you can sort of still see some things, like, hey, the laugh sounds a little unnatural.
01:26:54
They're sort of looking a little bit in a different direction you might expect. Remember when they did the first Avatar for me? I was looking at it, and something just seemed odds, right? It was just sort of moving the mouth and the audio was coming out, and it sounded pretty much like me.
01:27:06
But what I noticed was that the Adam's apple, as it was moving didn't seem to correspond with what I was saying, and so that was sort of off in the Avatar that they created for me, but I'm sure that they're going to fix that over time. And to all the opportunities for misinformation and other problematic things that we might see online and on social media.
01:27:26
You know, there's a lot going on in that space, but just to give you a sense again of the technology that's available. And one of the last things I want to show, that's really interesting. Um, is one of my favorite clips from South Park. It's a bit of an odd clip from South Park, because it's like 30 seconds, and there's no swearing.
01:27:45
Uh, which is really unusual for the show. Uh, I'll just walk you through it. Sorry, I can't. I can't play for you with audio. Uh, this is a clip where a person here in the middle let me just actually turn off the audio for a little while. I'm going to get distracted.
01:27:59
Uh, so the gentleman here on the right is called tweak. He's very Twitchy because he drinks a lot of coffee. And what he's noticed is that everybody's Underpants are missing in South Park. And so they followed the gentleman here on the right, which is, which is a gnome to their case.
01:28:15
And they try to find out what's going on. Why are they stealing everybody's underground, right? So now there's a conversation going back and forth to sort of narrate this for you. Going back and forth is why is this happening and so? This guy's, like, I'm not sure. So let me ask the other gnomes in the cave.
01:28:31
What's going on? What are we doing with all these Underpants? And they're going to show this in a second, so we'll just stop it there. One second. Here we go. So, this is their plan. The plan is phase one select underpants, phase two question mark phase three profit. Okay, this is a video clip that works at pretty much any business class because there's so many instances where somebody thinks, oh, we'll just do this, and that will lead to profit.
01:29:00
But how do they actually get from step one to the step of profit is very unclear, and I think that's a lot of the area that we're in right now is where there's lots of companies seeing the potential for generative AI within their organizations. Like, oh, let's just license this by this higher people.
01:29:15
Do whatever. Or even fire people with the anticipation we're going to have. You're going to be able to leverage gender wbi, but they just don't know quite how to get to the profit point. Okay. Um. And I think that's the biggest problem at the moment. Is that people don't have a good enough sense of exactly how to give it to work, and they might think, well, this can just do exactly the entire job of this particular individual.
01:29:38
And I don't think we're anywhere close to that yet. We are close to is saying these are a set of tasks that are that are connected to a human, but has a particular job and maybe 50 in some cases, maybe 80. In other cases, maybe 20, and even other cases could be automated, either partially or completely regenered.
01:29:56
Or can I today? Now, that's it. Are there lots of instances that you will see in the news that, say, hey, we? We tried this, and it didn't work. Yes, why is that? Because people are stupid. That's that. That's my line. I'm gonna use this in a keynote in a couple of weeks.
01:30:15
The reason is not, has nothing to do with gender to the app. It has to do with people thinking about. Hey, I can, I can invest in this new software, do this thing, but they don't have a good enough plan and a timeline how to get it to actually work.
01:30:28
As an example, I was just online with six people that together helped manage the master of science and business analytics program. And we're getting lots of complaints from students about them not being able to get their i-20. These are international students that want to come to the US to study with us, and they can't get this document from our International office if you will, because they they don't have the capacity to go through the more than a thousand applications for this I-20 form.
01:30:55
And so, what's the problem? The problem is, they just came up with a new system that was supposed to improve all the processes. And apparently, they can't get it to work. And this has absolutely nothing to do with generative. Yeah, it has to do with sort of humans and planning and the inability for people to come up with really good systems that actually work from day one.
01:31:14
Those of you that work at UCSD, I've seen this repeatedly with new systems that have been brought online that nobody seems to be able to use particularly well, and it takes a long, long time and an awful lot of money before it actually starts to work well. I think this is another example of that.
01:31:29
Now, generative VI is not immune to this, either. If you think that you could just sort of throw a bunch of stuff at the wall and magically, it will start to do all the things that it has the potential to do. That's not going to happen. And so, where I think and I'll just close with that because we're almost out of time and I want to give you some additional opportunity to ask questions?
01:31:47
I just taught a class on January VI for business and the first thing we did that. I asked the students to do is to to imagine a process that they might work in in an organization that they would like to join at some point in the future, think about a process could be anything could be in.
01:32:01
Finance could be a marketing could be. Think about that process, and then start thinking about what parts of that is it possible for you to automate with generator video. Okay, so what, I think, a lot of people are right now. Thinking about gender review? I wanted to do this.
01:32:12
One thing I wouldn't have been to build a tool that does this thing. I think it's much better to think about a larger process and figure out where gender VI could be most effective and start with sort of a small win. Try to find something that it can do well, that's easy for a human to Monitor and check for quality.
01:32:26
And then, if that works well, try to build it out from there, but sort of decentralized approach. Where from on high somebody's going to say, we shall now generative AI the heck out of our entire organization. I think that is doomed to fail in the vast majority of cases, sort of a bottom-up approach where people get to say this is how I work.
01:32:45
This is how my work gets done. Can you help me figure that out, that small specific piece how you might even be able to, like, I said, write some coach yourself with the help of generative. Yeah, right to sort of help automate some parts of processes. Don't put any of that into production because you don't want to sort of have major security holes or people hacking your organization, but think about for yourself and what you do do on a day-to-day basis.
01:33:07
What parts of that could generated?
01:33:15
I hope they didn't freak anybody out. I hope you came in at least as enthusiastic about life as you as you do now, uh, with that, I'll sort of close off again. I have many more slides. This is definitely an area I'm very, very interested in. It has lots of positive potential.
01:33:30
Also, a lot of negative potential, and it's going to be up to all of us to figure out how to get the good parts and to avoid some of the worst parts. Okay, thank you for your attention. Uh, I'm gonna stick around for at least another 5 or 10 minutes.
01:33:44
Any questions you have? Feel free to ask if you're done for the day. You want to go have some food or some dinner. Feel free, but I'll stick around and ask. Answer any questions you might have. You're a club! Thank you so much! Thank you! You're very welcome! Your clothes was so fun!
01:33:59
Thanks for being so animated! I love it! I love your passion for it, and I thought you might find this interesting. My brother's a robotic software guy, who is? So shockingly autistic, and he said GPT is actually helping him become more human.
```
